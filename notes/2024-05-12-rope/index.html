<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Bertrand's Notes</title>
        <meta name="description" content="Notes & Thoughts">
        <meta name="author" content="Bertrand Thia">
        <link rel="canonical" href="https://bt2513.github.io/" />
        <link rel="icon" href="/assets/img/corgi-icon.svg">
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css" rel="stylesheet"
          integrity="sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65" crossorigin="anonymous">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Inter:wght@600&display=swap" rel="stylesheet">
        <link rel="stylesheet" href="/assets/css/style.css">

    </head>

    <body>
        <!-- NAVBAR -->
        <nav class="navbar navbar-expand-lg">
          <div class="container">
            <a class="navbar-brand" href="https://bt2513.github.io/"><img src="/assets/img/corgi-icon.svg" alt="Bertrand's Notes" width="30" height="30"></a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent"
              aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
              <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav ms-auto mb-2 mb-lg-0">
                    <li class="nav-item">
                        <a class="nav-link" href="https://github.com/bt2513" target="_blank">
                            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>
                        </a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="https://www.linkedin.com/in/bertrand-thia/" target="_blank">
                            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/></svg>
                        </a>
                    </li>
                </ul>
            </div>
          </div>
        </nav>
      
        <!-- MAIN CONTENT -->
        <div class="container mt-5">
          <!-- INTRO -->
          <h2 class="mb-4">ðŸª¢ RoPE: Rotary Position Embedding</h2>
          <h6>
            <a href="https://arxiv.org/pdf/2104.09864.pdf"><u>Paper</u></a> 
            | Roformer: Enhanced Transformer with Rotary Position Embedding
          </h6>
          <hr>

          <div class="container article">
            <h3 class="mt-5">What is RoPE?</h3>
            <ul>
              <li>A technique used to <b>encode positional information</b> of tokens in the input sequence of Transformer-based language models</li>
              <li>Essential for understanding the <b>order</b> and meaning of tokens in large language models (e.g. GPT-3, BERT, etc.)</li>
              <li>The self-attention mechanism used in Transformers does <b>not</b> inherently capture the positional information of tokens</li>
            </ul>

            <h3>Why is RoPE used?</h3>
              <ul>
                <li>
                  Overcomes limitations of <i>fixed</i> and <i>learned</i> positional embeddings
                  <ul>
                    <li><i>Fixed</i> positional embeddings are predefined vectors that are added to the token embeddings at the input layer, 
                      encoding the position of each token in the sequence
                    </li>
                    <li><i>Learned</i> positional embeddings are trainable vectors that are learned during the model training process,
                      allowing the model to capture more complex positional patterns
                    </li>
                  </ul>
                </li>
                <li><b>Efficiency</b>: Saves memory usage and computation. It does not require storing or learning separate 
                    positional embeddings for each position in the sequence. This is particularly beneficial for long sequences or very 
                    large vocabularies</li>
                <li><b>Generalization</b>: Can generalize to sequence lengths longer than those seen during training, as the sinusoidal encodings 
                  can be computed on-the-fly for any position. Fixed and learned positional embeddings are limited to the 
                  maximum sequence length seen during training</li>
                <li><b>Relative Positional Information</b>: Can capture relative positional information between tokens, which is useful for tasks 
                    like machine translation or language generation, where the relative position of tokens is important</li>
                <li><b>Performance</b>: Has shown improved performances on various natural language processing tasks, 
                    particularly for long sequences or tasks that require capturing long-range dependencies.</li>
              </ul>

            <div class="callout info my-4">
              ðŸ’¡ Overall, RoPE is a more efficient and effective way of encoding positional information in large language models, 
              providing better generalization, capturing relative positional information, and improving model performance, especially 
              for long sequences or tasks that require capturing long-range dependencies.
            </div>

            <h3>How does RoPE work?</h3>
              <u>Steps in RoPE</u>
                <ol>
                  <li><b>Initialize Frequency Array</b>: 
                  Array of frequencies initialized using an exponential scaling function.
                  These frequencies serve as rotation factors
                  </li>

                  <li><b>Position-Based Scaling</b>: 
                  Positions of tokens in the sequence are scaled by the frequency array.
                  Scaled positions are used for rotating the embeddings (instead of addition)
                  </li>

                  <li><b>Construct Rotary Matrix</b>:
                  Rotary matrix created by stacking sine and cosine values of scaled angles.
                  This matrix is used to rotate the original embeddings
                  </li>

                  <li><b>Rotate Embeddings</b>:
                  Rotary matrix reshaped to match the model's embedding dimension.
                  Reshaped matrix multiplied with original <i>query</i> and <i>key</i> embeddings.
                  Embeddings rotated based on their positions
                  </li>
                </ol>

              <u>Key Aspects of RoPE</u>
                <ul>
                  <li><b>Variable Rotation Speed</b>:
                  Different dimensions in embeddings rotated at different speeds (determined by frequencies).
                  Can be visualized as different clock hands rotating at distinct speeds
                  </li>

                  <li><b>Dot Product Significance</b>:
                  Embeddings rotated to similar angles have high dot product (indicating positional closeness).
                  Dot product diminishes as relative distance increases (encoding relative positional information)
                  </li>

                  <li><b>Flexible Sequence Lengths</b>:
                  Can generalize to arbitrary sequence lengths by rotating embeddings accordingly
                  (unlike fixed positional embeddings)
                  </li>

                  <li><b>Relative Position Encoding</b>:
                  Naturally incorporates relative position information into self-attention mechanism.
                  Enables capturing dependencies between tokens based on their relative positions
                  </li>
                </ul>

              <img class="my-4" src="/notes/2024-05-12-rope/rope-formula.png" width="50%" height="25%">

            <h3>Conclusion</h3>
              <ul>
                <li>Really enjoyed the paper: clear and rigorous mathematical proofs</li>
              </ul>

              A few final words from the authors:
              <div class="callout quote">
                "Despite the fact that we mathematically format the relative position relations as rotations under 2D sub-spaces, 
                there lacks of thorough explanations on why it converges faster than baseline models that incorporates other position 
                encoding strategies."
              </div>
              <div class="callout quote">
                "Although we have proved that our model has favourable property of long-term decay for intern-token products, 
                which is similar to the existing position encoding mechanisms, our model shows superior performance on long texts 
                than peer models, we have not come up with a faithful explanation."
              </div>
          </div>
        </div>

        <!-- FOOTER -->
        <footer class="footer">
            <div class="container text-center">
                <span>&copy; 2024 <a href="https://bt2513.github.io/">Bertrand's Notes</a></span>
            </div>
        </footer>

        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-kenU1KFdBIe4zVF0s0G1M5b4hcpxyD9F7jL+jjXkk+Q2h455rYXK/7HAuoJl+0I4" crossorigin="anonymous">
        </script>
      </body>

</html>